"""
Video Analysis Route
ROI Detection + Task Classification
"""

from flask import Blueprint, request, jsonify, current_app
import os
import cv2
from werkzeug.utils import secure_filename
import sys
import threading
import json
import time

# Add services directory to path
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

from services.roi_detector import MovementBasedROI, ROIResult
from services.task_classifier import TaskClassifier, TaskClassificationResult
from services.mediapipe_processor import MediaPipeProcessor
from services.metrics_calculator import MetricsCalculator
from services.updrs_scorer import UPDRSScorer
from services.interpretation_agent import InterpretationAgent
from services.progress_tracker import init_analysis, update_step, complete_analysis, fail_analysis
from services.visualization_data_generator import generate_visualization_data, detect_events
from agents.orchestrator import OrchestratorAgent
from domain.context import AnalysisContext
from dataclasses import asdict

bp = Blueprint('analyze', __name__, url_prefix='/api')


def allowed_file(filename):
    """Check if file extension is allowed"""
    ALLOWED_EXTENSIONS = {'mp4', 'avi', 'mov', 'webm', 'mkv'}
    return '.' in filename and filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS


def process_video_background(video_path, video_id, patient_id, manual_test_type, app_config, scoring_method='ensemble', ml_model_type='rf'):
    """
    Background task for video analysis using Multi-Agent Orchestrator
    """
    try:
        print(f"\n{'='*50}")
        print(f"Processing video: {os.path.basename(video_path)} (ID: {video_id})")
        print(f"{'='*50}\n")

        # Initialize Orchestrator
        orchestrator = OrchestratorAgent()
        
        # Define progress callback
        def progress_callback(step_name, status, **kwargs):
            update_step(video_id, step_name, status, **kwargs)

        # Create Context with scoring configuration
        ctx = AnalysisContext(
            video_path=video_path, 
            video_id=video_id,
            scoring_method=scoring_method,
            ml_model_type=ml_model_type
        )
        
        # Run Analysis
        ctx = orchestrator.process(ctx, on_progress_update=progress_callback)
        
        if ctx.error:
            raise Exception(ctx.error)

        # Post-processing for Visualizations & Response Formatting
        print("\nStep 4: Generating visualizations...")
        
        # Extract data from context
        landmarks = ctx.skeleton_data.get("landmarks", [])
        metrics = ctx.kinematic_metrics
        updrs_result = ctx.clinical_scores # This is UPDRSScore object
        ai_result = ctx.report # This is Report object
        
        # Convert landmarks to dict for visualization
        # Note: landmarks are already dicts (converted by VisionAgent using asdict)
        frames_data = []
        for lf in landmarks:
            frames_data.append({
                "frame": lf.get("frame_number", 0),
                "timestamp": lf.get("timestamp", 0.0),
                "keypoints": lf.get("landmarks", [])
            })
            
        # Heatmap (Already generated by VisionAgent)
        update_step(video_id, "heatmap", "in_progress")
        heatmap_path = ctx.vision_meta.get("heatmap_path")
        update_step(video_id, "heatmap", "completed")
        
        # Temporal Map (Already generated by VisionAgent)
        update_step(video_id, "temporal_map", "in_progress")
        temporal_path = ctx.vision_meta.get("trajectory_map_path")
        update_step(video_id, "temporal_map", "completed")
        
        # Attention Map (Not yet in VisionAgent, keep or skip?)
        # For now, we skip generating it to save time if it's not critical, 
        # or we could move it to VisionAgent later. 
        # Let's keep it if it was there, but we need viz_service.
        # Since we removed viz_service init, let's skip it for now or init it just for this.
        # Actually, let's remove it to streamline.
        attention_path = None

        # Overlay Video - Use skeleton video generated by VisionAgent
        skeleton_video_path = ctx.vision_meta.get("skeleton_video_path")
        if skeleton_video_path and os.path.exists(skeleton_video_path):
            skeleton_video_url = f"/files/{os.path.basename(skeleton_video_path)}"
        else:
            # Fallback to original video if skeleton not available
            skeleton_video_url = f"/files/{os.path.basename(video_path)}"
        update_step(video_id, "overlay_video", "completed", result_url=skeleton_video_url)

        # Prepare Response
        
        # Convert objects to dicts
        updrs_dict = updrs_result if updrs_result else None
        
        ai_interpretation = None
        if ai_result:
            ai_interpretation = {
                "summary": ai_result.summary_for_patient,
                "explanation": ai_result.summary_for_clinician,
                "recommendations": ai_result.recommendations
            }

        # Reasoning Log conversion - transform to frontend format
        reasoning_log = []
        for step in ctx.reasoning_log:
            step_dict = step.model_dump(mode='json')
            # Map backend fields to frontend fields
            reasoning_log.append({
                "agent": step_dict.get("step", "unknown"),  # Backend 'step' -> Frontend 'agent'
                "step": step_dict.get("step", ""),  # Keep step for display
                "content": step_dict.get("message", ""),  # Backend 'message' -> Frontend 'content'
                "timestamp": step_dict.get("timestamp", ""),
                "meta": step_dict.get("meta")
            })

        # Generate visualization data for charts
        fps = ctx.vision_meta.get("fps", 30.0)
        # Get gait analysis from gait_cycle_data (populated by GaitCycleAgent)
        gait_analysis = getattr(ctx, 'gait_cycle_data', None)
        visualization_data = generate_visualization_data(
            landmark_frames=frames_data,
            gait_analysis=gait_analysis,
            fps=fps
        )

        # Detect clinically relevant events
        event_task_type = "finger_tapping" if "finger" in ctx.task_type.lower() or "tapping" in ctx.task_type.lower() else "gait"
        detected_events = detect_events(
            landmark_frames=frames_data,
            gait_analysis=gait_analysis,
            fps=fps,
            task_type=event_task_type
        )

        response = {
            "success": True,
            "id": video_id,
            "patient_id": patient_id,
            "video_type": ctx.task_type,
            "auto_detected": manual_test_type is None,
            "confidence": ctx.vision_meta.get("confidence", 0.0),
            "roi": {
                "x": ctx.vision_meta.get("roi", (0,0,0,0))[0],
                "y": ctx.vision_meta.get("roi", (0,0,0,0))[1],
                "w": ctx.vision_meta.get("roi", (0,0,0,0))[2],
                "h": ctx.vision_meta.get("roi", (0,0,0,0))[3]
            },
            "motion_analysis": {
                "motion_pattern": ctx.vision_meta.get("motion_pattern"),
                "motion_area_ratio": ctx.vision_meta.get("motion_area_ratio"),
                "body_part": ctx.vision_meta.get("body_part")
            },
            "reasoning": ctx.vision_meta.get("reasoning", ""), # Legacy single string
            "reasoning_log": reasoning_log, # NEW: Full log
            "video_metadata": {
                "width": 0, # TODO: Get from meta
                "height": 0,
                "fps": ctx.vision_meta.get("fps", 0),
                "duration": 0,
                "total_frames": ctx.vision_meta.get("frame_count", 0)
            },
            "metrics": metrics,
            "skeleton_data": {
                "total_frames": len(landmarks),
                "detection_rate": 0, # Calculate if needed
                "mode": "pose" if ctx.task_type in ["gait", "leg_agility"] else "hand",
                "skeleton_video_url": skeleton_video_url
            },
            "scoring_method": ctx.scoring_method,
            "ml_model_type": ctx.ml_model_type,
            "updrs_score": updrs_dict,
            "ai_interpretation": ai_interpretation,
            "events": detected_events,
            "visualization_urls": {
                "heatmap": f"/files/{os.path.basename(heatmap_path)}" if heatmap_path else None,
                "temporal_map": f"/files/{os.path.basename(temporal_path)}" if temporal_path else None,
                "attention_map": None
            },
            "visualization_data": visualization_data,
            "gait_cycle_analysis": gait_analysis  # Include raw gait cycle data
        }

        # Save result
        result_path = os.path.join(app_config['UPLOAD_FOLDER'], f"{video_id}_result.json")
        with open(result_path, 'w', encoding='utf-8') as f:
            json.dump(response, f, ensure_ascii=False, indent=2)

        # Mark analysis as completed
        update_step(video_id, "updrs_calculation", "completed")
        update_step(video_id, "ai_interpretation", "completed")
        complete_analysis(video_id)

        print(f"\n{'='*50}")
        print(f"Analysis Complete! (ID: {video_id})")

    except Exception as e:
        print(f"\nError during analysis:")
        print(f"  {str(e)}\n")
        import traceback
        with open('error.log', 'w') as f:
            f.write(f"Error: {str(e)}\n")
            traceback.print_exc(file=f)
        traceback.print_exc()
        fail_analysis(video_id, str(e))


@bp.route('/analyze', methods=['POST'])
def start_analysis():
    """
    Start asynchronous video analysis
    """
    try:
        # Check if video file is present
        if 'video_file' not in request.files:
            return jsonify({
                "success": False,
                "error": "No video file provided"
            }), 400

        video_file = request.files['video_file']

        if video_file.filename == '':
            return jsonify({
                "success": False,
                "error": "No file selected"
            }), 400

        if not allowed_file(video_file.filename):
            return jsonify({
                "success": False,
                "error": "Invalid file type. Allowed: mp4, avi, mov, webm, mkv"
            }), 400

        # Generate unique video_id for progress tracking
        import time
        filename = secure_filename(video_file.filename)
        video_id = f"{os.path.splitext(filename)[0]}_{int(time.time())}"
        
        # Save video file
        # Note: We need to save it here before starting the thread
        video_path = os.path.join(current_app.config['UPLOAD_FOLDER'], f"{video_id}_{filename}")
        video_file.save(video_path)

        # Initialize progress tracking
        init_analysis(video_id, task_type="auto_detect")

        # Get optional parameters
        patient_id = request.form.get('patient_id', 'unknown')
        manual_test_type = request.form.get('test_type', None)
        # Feature mismatch fixed (2024-12-09): 27 features correctly configured
        # Ensemble scoring now available: combines Rule-based + ML predictions
        scoring_method = request.form.get('scoring_method', 'ensemble')  # rule, ml, ensemble
        ml_model_type = request.form.get('ml_model_type', 'rf')  # rf or xgb
        
        # Start background thread
        thread = threading.Thread(
            target=process_video_background,
            args=(
                video_path,
                video_id,
                patient_id,
                manual_test_type,
                current_app.config.copy(),
                scoring_method,
                ml_model_type
            )
        )
        thread.daemon = True
        thread.start()

        return jsonify({
            "success": True,
            "message": "Analysis started",
            "id": video_id,
            "status": "in_progress"
        }), 202

    except Exception as e:
        print(f"Error starting analysis: {e}")
        return jsonify({
            "success": False,
            "error": str(e)
        }), 500


@bp.route('/analysis/result/<video_id>', methods=['GET'])
def get_analysis_result(video_id):
    """
    Get the final result of an analysis
    """
    result_path = os.path.join(current_app.config['UPLOAD_FOLDER'], f"{video_id}_result.json")
    
    if not os.path.exists(result_path):
        return jsonify({
            "success": False,
            "error": "Result not found or analysis not complete"
        }), 404
        
    try:
        with open(result_path, 'r', encoding='utf-8') as f:
            result = json.load(f)
        return jsonify(result)
    except Exception as e:
        return jsonify({
            "success": False,
            "error": f"Error reading result: {str(e)}"
        }), 500


@bp.route('/analyze-status/<analysis_id>', methods=['GET'])
def get_analysis_status(analysis_id):
    """
    Get analysis status (for async processing - future implementation)
    """
    # This is now redundant with progress_tracker but kept for compatibility
    return jsonify({
        "success": False,
        "error": "Use /api/analysis/progress/<video_id> instead"
    }), 301

